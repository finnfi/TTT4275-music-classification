{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9317204",
   "metadata": {},
   "source": [
    "# TTK4275 Music Classification Project\n",
    "\n",
    "## Code: part-by-part\n",
    "\n",
    "This notebook contains the majority of the relevant code. For the code that finds the optimal hyperparameters in part 4, please refer to the attached code folder. This notebook has to be run from top to buttom, in that order, to guarantee the right result in each cell. E.g. if you only want to look at part 4, you have to run the previously parts first. This can easily be done from the \"cell\" dropdown menu in the menu bar on top. \n",
    "\n",
    "Tip: Do *Cell -> Run all* in the beggining to have all code run before reading.  \n",
    "\n",
    "### Part 0: Imports and initialising of features\n",
    "\n",
    "First we need to initialise a feature structure and load the feature data into it. \n",
    "First some necessary imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ddc27",
   "metadata": {},
   "source": [
    "Then we need to define a SongFeature stuct and define some reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e970b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongFeatures:\n",
    "    def __init__(self, arr):\n",
    "        self.Track_ID,self.File,self.zero_cross_rate_mean,self.zero_cross_rate_std,self.rmse_mean,self.rmse_var,self.spectral_centroid_mean,self.spectral_centroid_var,\\\n",
    "        self.spectral_bandwidth_mean,self.spectral_bandwidth_var,self.spectral_rolloff_mean,self.spectral_rolloff_var,self.spectral_contrast_mean,self.spectral_contrast_var,self.spectral_flatness_mean,\\\n",
    "        self.spectral_flatness_var,self.chroma_stft_1_mean,self.chroma_stft_2_mean,self.chroma_stft_3_mean,self.chroma_stft_4_mean,self.chroma_stft_5_mean,self.chroma_stft_6_mean,self.chroma_stft_7_mean,\\\n",
    "        self.chroma_stft_8_mean,self.chroma_stft_9_mean,self.chroma_stft_10_mean,self.chroma_stft_11_mean,self.chroma_stft_12_mean,self.chroma_stft_1_std,self.chroma_stft_2_std,self.chroma_stft_3_std,\\\n",
    "        self.chroma_stft_4_std,self.chroma_stft_5_std,self.chroma_stft_6_std,self.chroma_stft_7_std,self.chroma_stft_8_std,self.chroma_stft_9_std,self.chroma_stft_10_std,self.chroma_stft_11_std,self.chroma_stft_12_std,\\\n",
    "        self.tempo,self.mfcc_1_mean,self.mfcc_2_mean,self.mfcc_3_mean,self.mfcc_4_mean,self.mfcc_5_mean,self.mfcc_6_mean,self.mfcc_7_mean,self.mfcc_8_mean,self.mfcc_9_mean,self.mfcc_10_mean,self.mfcc_11_mean,\\\n",
    "        self.mfcc_12_mean,self.mfcc_1_std,self.mfcc_2_std,self.mfcc_3_std,self.mfcc_4_std,self.mfcc_5_std,self.mfcc_6_std,self.mfcc_7_std,self.mfcc_8_std,self.mfcc_9_std,self.mfcc_10_std,self.mfcc_11_std,self.mfcc_12_std,\\\n",
    "        self.GenreID,self.Genre,self.Type = arr\n",
    "        \n",
    "def readGenreClassData(file_path):\n",
    "    '''\n",
    "    Read genre class data from path_to_file and returns a dict of SongFeatures [from TrackID to SongFeature]\n",
    "    '''\n",
    "    with open(file_path) as f:\n",
    "        f.readline() #Remove description\n",
    "        lines = f.readlines()\n",
    "    dict_of_SF = dict()\n",
    "    for str in lines:\n",
    "        arr = re.split(r'\\t+', str.rstrip('\\n'))\n",
    "        arr[0] = int(arr[0])\n",
    "        arr[-3] = int(arr[-3])\n",
    "        for i in range(2,len(arr)-3):\n",
    "            arr[i] = float(arr[i])\n",
    "        dict_of_SF[arr[0]] = SongFeatures(arr)\n",
    "    return dict_of_SF\n",
    "\n",
    "def getPointsAndClasses(songs_dict, features, classes, type):\n",
    "    '''\n",
    "    input:  songs_dict  - a dict from id to SongFeature\n",
    "            features    - array of features to be used\n",
    "            classes     - array of classes to be used\n",
    "            type        - \"Train\" or \"Test\"\n",
    "    output: X, y, id_list - X is a np.array of size (N_POINTS, N_FEATURES), \n",
    "                            y is a np.array of class ids with size N_POINTS, \n",
    "                            id_list is id to corresponding song\n",
    "    '''\n",
    "    n_features  = len(features)\n",
    "\n",
    "    X = np.empty([0,n_features])\n",
    "    y = np.empty(0,dtype=np.int16)\n",
    "    id_list = []\n",
    "    for song in songs_dict.values():\n",
    "        if song.Type == type and song.Genre in classes:\n",
    "            x = np.zeros([1,n_features])\n",
    "            for i in range(n_features):\n",
    "                x[0,i] = song.__dict__[features[i]]\n",
    "            X = np.append(X,x,axis = 0)\n",
    "            y = np.append(y,song.GenreID) \n",
    "            id_list.append(song.Track_ID)\n",
    "    \n",
    "    return X, y, id_list\n",
    "\n",
    "def genre_string_to_id(genre):\n",
    "    '''\n",
    "    input: genre in string format\n",
    "    output: genre id\n",
    "    '''\n",
    "    genres = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]\n",
    "    return genres.index(genre)\n",
    "\n",
    "def genre_id_to_string(genre_id):\n",
    "    '''\n",
    "    input: genre_id\n",
    "    output: genre in string format\n",
    "    '''\n",
    "    genres = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]\n",
    "    return genres[genre_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d1c80",
   "metadata": {},
   "source": [
    "Next step is to actually read from file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676da571",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_dict = readGenreClassData(\"Data/GenreClassData_30s.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d2a2f",
   "metadata": {},
   "source": [
    "### Part 1: Design of KNN classifier\n",
    "\n",
    "This notebook only contains our own KNN classifier. The sklearn implementation can be found in the attached code folder. \n",
    "\n",
    "First we designed a KNN classifier class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aac12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, X, y, ids_list, features, k, input_normalisation_type = \"\"):\n",
    "        '''\n",
    "        X               : point nd.array of dimension (N_POINTS, N_FEATURES)\n",
    "        y               : nd.array of class labels\n",
    "        ids_list        : array of correspongin track ids \n",
    "        features        : array of feature strings\n",
    "        k               : int \n",
    "        should_normalise: \"\" -> no normalisation, \"z_score\", \"min_max\"\n",
    "        '''\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        self.ids_list = ids_list.copy()\n",
    "        self.classes_id = list(np.unique(y))\n",
    "        self.classes = [genre_id_to_string(i) for i in self.classes_id]\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.k = k\n",
    "        self.features = features\n",
    "        self.dim = len(features)\n",
    "        self.num_points = np.size(X,0)\n",
    "        \n",
    "        #PCA variables:\n",
    "        self.isPCAused = False\n",
    "        self.pca_n_components = 0\n",
    "        \n",
    "        #Normalisation\n",
    "        # Keep track of mean, sd, min and max for each feature to normalise\n",
    "        self.features_mean_sd = [] # array of tuple (mean, sd)\n",
    "        self.features_min_max = [] # array of tuple (min, max)\n",
    "        \n",
    "        self.input_normalisation_type = input_normalisation_type\n",
    "        if input_normalisation_type == \"z_score\":\n",
    "            self.z_score_normalise()\n",
    "        elif input_normalisation_type == \"min_max\":\n",
    "            self.min_max_normalise()\n",
    "            \n",
    "    def z_score_normalise(self):\n",
    "        '''\n",
    "        Normalises features using z-score normalisation\n",
    "        '''\n",
    "        for i in range(self.dim):\n",
    "            mean = np.mean(self.X[:,i])\n",
    "            var = np.var(self.X[:,i])\n",
    "            sd = np.sqrt(var)\n",
    "            self.X[:,i] = (self.X[:,i]-mean)/sd\n",
    "            self.features_mean_sd.append((mean,sd))\n",
    "\n",
    "    def min_max_normalise(self):\n",
    "        '''\n",
    "        Normalises features using min-max normalisation\n",
    "        '''\n",
    "        for i in range(self.dim):\n",
    "            min = np.min(self.X[:,i])\n",
    "            max = np.max(self.X[:,i])\n",
    "            diff = max - min\n",
    "\n",
    "            self.X[:,i] = (self.X[:,i]-min)/diff\n",
    "            self.features_min_max.append((min,max))\n",
    "        \n",
    "    def doPCA(self, n_components):\n",
    "        '''\n",
    "        Transform points using PCA analysis\n",
    "        Changes self.points\n",
    "        '''\n",
    "        self.isPCAused = True\n",
    "        self.pca_n_components = n_components\n",
    "\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.X = self.pca.fit_transform(self.X)\n",
    "\n",
    "    def classify(self, x):\n",
    "        '''\n",
    "        input: x of type np.array with dim = k (NOT normalised)\n",
    "        output: genre ID\n",
    "        '''\n",
    "        #normalise input if normalisation is enabled\n",
    "        if self.input_normalisation_type == \"z_score\":\n",
    "            for i in range(self.dim):\n",
    "                x[i] = (x[i]-self.features_mean_sd[i][0])/self.features_mean_sd[i][1]\n",
    "        elif self.input_normalisation_type == \"min_max\":\n",
    "            for i in range(self.dim):\n",
    "                min = (self.features_min_max[i][0])\n",
    "                max = self.features_min_max[i][1]\n",
    "                diff = max-min\n",
    "                x[i] = (x[i]-min)/diff\n",
    "        \n",
    "        #Do PCA transform if PCA is anabled\n",
    "        if self.isPCAused:\n",
    "            x = self.pca.transform(x.reshape(1, -1))\n",
    "\n",
    "        # Calculate distances\n",
    "        difference = self.X-x\n",
    "        distances = np.sum(difference*difference,axis=1)\n",
    "        # k smallest distances: \n",
    "        #function gives array of indexes\n",
    "        index_k_nearest_points = np.argpartition(distances, self.k)[:self.k] \n",
    "        distance_k_nearest_points = distances[index_k_nearest_points]\n",
    "\n",
    "        # Find genres \n",
    "        genres_k_nearest_points = []\n",
    "        for i in index_k_nearest_points:\n",
    "            genres_k_nearest_points.append(self.y[i])\n",
    "        \n",
    "        # Combine data [[index, distance,genre], [index,distance,genre], ...]\n",
    "        k_nearest_points = [[index_k_nearest_points[i],distance_k_nearest_points[i],\n",
    "                            genres_k_nearest_points[i]] for i in range(self.k)]\n",
    "        \n",
    "        #Count genres\n",
    "        genres_count = dict()\n",
    "        for point in k_nearest_points:\n",
    "            genres_count[point[2]] = genres_count.get(point[2],0) + 1\n",
    "        genres_count = list(genres_count.items())\n",
    "        genres_count = sorted(genres_count, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        #Find genre(s) with most entries, delete the others\n",
    "        for i in range(1,len(genres_count)):\n",
    "            if genres_count[i][1] < genres_count[i-1][1]:\n",
    "                genres_count = genres_count[:i]\n",
    "                break\n",
    "        \n",
    "        genres = [genre[0] for genre in genres_count]\n",
    "\n",
    "        #Find all points belonging to these genres\n",
    "        points_to_consider = []\n",
    "        for point in k_nearest_points:\n",
    "            if point[2] in genres:\n",
    "                points_to_consider.append(point)\n",
    "\n",
    "        #Sort points to consider\n",
    "        points_to_consider = sorted(points_to_consider, key=lambda x:x[1])\n",
    "\n",
    "        #Return genre ID with smallest distance\n",
    "        return points_to_consider[0][2]\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, ids_list_test):\n",
    "        '''\n",
    "        input: \n",
    "        X_test                  : point nd.array of dimension (N_POINTS, N_FEATURES)\n",
    "        y_test                  : nd.array of class labels\n",
    "        ids_list_test           : array of correspongin track ids \n",
    "\n",
    "        output:\n",
    "        confusion_matrix        : 2D np array of ints with size (N_CLASSES, N_CLASSES)\n",
    "        confusion_matrix_list   : 2D array containg lists of classified song ids\n",
    "        er                      : Error rate          \n",
    "        '''\n",
    "        confusion_matrix_list = [[[] for j in range(self.num_classes)] for i in range(self.num_classes)]\n",
    "        confusion_matrix = np.zeros([self.num_classes,self.num_classes])\n",
    "\n",
    "        for i in range(len(y_test)):\n",
    "            genre_id = y_test[i]\n",
    "            classified_id = self.classify(X_test[i,:])\n",
    "            correct_index = self.classes_id.index(genre_id)\n",
    "            predicted_index = self.classes_id.index(classified_id)\n",
    "            confusion_matrix_list[correct_index][predicted_index].append(ids_list_test[i])\n",
    "            confusion_matrix[correct_index][predicted_index] +=  1\n",
    "        er = error_rate(confusion_matrix)\n",
    "        return confusion_matrix, confusion_matrix_list, er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32184dd",
   "metadata": {},
   "source": [
    "As well as defining an error rate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(confusion_matrix):\n",
    "    '''\n",
    "    input: confusion matrix as a 2D array\n",
    "    output: error rate (# of falsely classified points)/( # total points)\n",
    "    '''\n",
    "    total = np.sum(confusion_matrix)\n",
    "    correct = np.sum(np.diagonal(confusion_matrix))\n",
    "    \n",
    "    error_rate = (total-correct)/total\n",
    "\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44ad5d",
   "metadata": {},
   "source": [
    "Next we define features and genres we want to use, and extract the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74224",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\",\"mfcc_1_mean\",\"tempo\"]\n",
    "genres      = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9b462",
   "metadata": {},
   "source": [
    "Then create a KNN classifier with the training set and evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c943e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate of KNN without normalisation is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea88c7",
   "metadata": {},
   "source": [
    "With z score normalisation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"z_score\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate of KNN with z-score normalisation is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c719c9",
   "metadata": {},
   "source": [
    "With min-max normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3646be",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate of KNN with min-max is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717bc87",
   "metadata": {},
   "source": [
    "### Part 2: Separability\n",
    "\n",
    "Here we only look at 4 genres in this part, so we have to redefine some variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09eac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\",\"mfcc_1_mean\",\"tempo\"]\n",
    "genres      = [\"pop\", \"disco\",\"metal\",\"classical\"] \n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424d43d",
   "metadata": {},
   "source": [
    "Implement a histogram plotter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefb243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(X , y, features, mode = \"separat\"):\n",
    "    '''\n",
    "    input: \n",
    "    X       : Points np.array() with size (N_POINTS,N_FEATURES)\n",
    "    y       : Class IDs np.array() with size (N_POINTS)\n",
    "    features: array of features\n",
    "    mode    : \"separat\" or \"overlayed\"\n",
    "\n",
    "    output:axs_list\n",
    "    '''\n",
    "    # Define parameters:\n",
    "    n_bins = 14\n",
    "    n_features = len(features)\n",
    "    n_points = np.size(X,0)\n",
    "    # Get max and min for each feature\n",
    "    min_max_features = [] # Array with tuple for min and max for each feature\n",
    "    for i in range(n_features):\n",
    "        mm = [0,0]\n",
    "        mm[0] = np.min(X[:,i])\n",
    "        mm[1] = np.max(X[:,i])\n",
    "        min_max_features.append(mm)\n",
    "    \n",
    "    # Extract genres from dataset: \n",
    "    genre_dict = dict() # Dict from genre ID to np.array of points\n",
    "    for i in range(n_points):\n",
    "        genre_id = y[i]\n",
    "        genre_dict[genre_id] = np.append(genre_dict.get(genre_id, np.empty([0,n_features])), X[i,:].reshape(1,n_features), axis=0)\n",
    "\n",
    "    n_genres = len(genre_dict)\n",
    "\n",
    "    axs_list = [None]*n_features\n",
    "    if mode == \"separat\":\n",
    "        for j in range(n_features):\n",
    "            fig, axs = plt.subplots(n_genres,1,sharey=True, tight_layout=True)\n",
    "            plt.setp(axs, xlim=(min_max_features[j][0],min_max_features[j][1]), ylim=(0,20))\n",
    "            i = 0\n",
    "            for genre_id in genre_dict:\n",
    "                axs[i].hist(genre_dict[genre_id][:,j], bins = n_bins)\n",
    "                axs[i].set_title(genre_id_to_string(genre_id))\n",
    "                i = i + 1\n",
    "            fig.suptitle(features[j])\n",
    "            axs_list[j] = axs\n",
    "        \n",
    "\n",
    "\n",
    "    elif mode == \"overlayed\":\n",
    "        colors = [  (1,0,0,0.5),(0,1,0,0.5),(0,0,1,0.5),(1,1,0,0.5),(1,0,1,0.5),(0,1,1,0.5),\n",
    "                    (0.95,0.5,0.2,0.5), (0.25,0.75,0.3,0.5),(0.5,0,0,0.5),(0.25,0.4,0.8,0.5)]\n",
    "        for j in range(n_features):\n",
    "            fig, axs = plt.subplots(1,1,sharey=True, tight_layout=True)\n",
    "            plt.setp(axs, xlim=(min_max_features[j][0],min_max_features[j][1]), ylim=(0,20))\n",
    "            i = 0\n",
    "            for genre_id in genre_dict:\n",
    "                axs.hist(genre_dict[genre_id][:,j], bins = n_bins, label = genre_id_to_string(genre_id), fc = colors[i])\n",
    "                i = i + 1\n",
    "            fig.suptitle(features[j])\n",
    "            axs_list[j] = axs\n",
    "\n",
    "            #Add legends\n",
    "            axs.legend(loc='best', frameon=False)\n",
    "            fig.suptitle(features[j])\n",
    "\n",
    "    return axs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca834d",
   "metadata": {},
   "source": [
    "Plot the histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d53960",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\",\"mfcc_1_mean\",\"tempo\"]\n",
    "axs_list = plot_histogram(X_train, y_train, features, mode = \"overlayed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bfa47",
   "metadata": {},
   "source": [
    "Create a KNN classifier for each feature selection and evaluate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df649715",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\",\"mfcc_1_mean\",\"tempo\"]\n",
    "print(\"Chosen features: \", features)\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()\n",
    "\n",
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\",\"mfcc_1_mean\"]\n",
    "print(\"Chosen features: \", features)\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()\n",
    "\n",
    "features    = [\"spectral_rolloff_mean\",\"spectral_centroid_mean\"]\n",
    "print(\"Chosen features: \", features)\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()\n",
    "\n",
    "features    = [\"spectral_rolloff_mean\"]\n",
    "print(\"Chosen features: \", features)\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b7dea",
   "metadata": {},
   "source": [
    "*Extra task: separability with PCA*\n",
    "\n",
    "We also wanted to see how separat the four genres can become with three features. For that we used principal component analysis. \n",
    "\n",
    "First we had to create a 3D plotter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_feature_space(X , y, features):\n",
    "        '''\n",
    "        input\n",
    "        training_set: a GenreSet\n",
    "        genres: list of genres to plot\n",
    "        features: a list of size 3 with features to use\n",
    "\n",
    "        output\n",
    "        returns an ac object if dim==3, else nothing\n",
    "        '''\n",
    "        if (np.size(X,1) != 3):\n",
    "            return\n",
    "\n",
    "        # Define parameters:\n",
    "        n_features = len(features)\n",
    "        n_points = np.size(X,0)\n",
    "        # Get max and min for each feature\n",
    "        min_max_features = [] # Array with tuple for min and max for each feature\n",
    "        for i in range(n_features):\n",
    "            mm = [0,0]\n",
    "            mm[0] = np.min(X[:,i])\n",
    "            mm[1] = np.max(X[:,i])\n",
    "            min_max_features.append(mm)\n",
    "        \n",
    "        # Extract genres from dataset: \n",
    "        genre_dict = dict() # Dict from genre ID to np.array of points\n",
    "        for i in range(n_points):\n",
    "            genre_id = y[i]\n",
    "            genre_dict[genre_id] = np.append(genre_dict.get(genre_id, np.empty([0,n_features])), X[i,:].reshape(1,n_features), axis=0)\n",
    "        \n",
    "        #Init figure\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        for genre in genre_dict:\n",
    "            ax.scatter(genre_dict[genre][:,0],genre_dict[genre][:,1],genre_dict[genre][:,2],label=genre_id_to_string(genre))\n",
    "\n",
    "        #Set labels\n",
    "        ax.set_xlabel(features[0])\n",
    "        ax.set_ylabel(features[1])\n",
    "        ax.set_zlabel(features[2])\n",
    "\n",
    "        #Add legends\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140bea30",
   "metadata": {},
   "source": [
    "Then we had to do the PC anlysis, plot the 3D feature space and evaluate the confusion matrix with PCA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [ \"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "                \"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "                \"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "                \"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "                \"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "                \"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "                \"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "                \"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "\n",
    "ax = plot_3D_feature_space(X_train,y_train,[\"PC1\", \"PC2\", \"PC3\"])\n",
    "plt.show()\n",
    "\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "knn = KNNClassifier(X_train, y_train, ids_train, features, 5 ,\"min_max\")\n",
    "knn.doPCA(3)\n",
    "cm, cm_list, er = knn.evaluate(X_test.copy(),y_test.copy(),ids_test.copy()) \n",
    "print(\"The error rate is: \", er)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cfb31",
   "metadata": {},
   "source": [
    "### Part 3: Feature selection\n",
    "\n",
    "We implemented two feature selection methods: one iterative solution using k-fold cross validation and one using the ANOVA f-value as metric for best features. \n",
    "\n",
    "#### K-fold Cross Validation\n",
    "\n",
    "We work again on all genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres      = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d44e5",
   "metadata": {},
   "source": [
    "Define feature pools and make all combinations of pool 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pool_1 = [\"spectral_centroid_mean\",\"mfcc_1_mean\",\"spectral_rolloff_mean\",\"tempo\"]\n",
    "feature_pool_2 = [\"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "\"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "\"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "\"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "\"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "\"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "\"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "\"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "feature_pool_1_combinations = list(combinations(feature_pool_1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7c0c4",
   "metadata": {},
   "source": [
    "Define an iterative function that checks all possible feature combinations, calculate average errors on folds and returns the best function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204552ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(feature_pool_1_combinations, feature_pool_2):\n",
    "    best_error_rate = 1\n",
    "    chosen_features = []\n",
    "    \n",
    "    n_splits = 4\n",
    "\n",
    "    for comb in feature_pool_1_combinations:\n",
    "        for feature in feature_pool_2:\n",
    "            features = list(comb) + [feature]\n",
    "            X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            skf = StratifiedKFold(n_splits=n_splits)\n",
    "            skf.get_n_splits(X_train, y_train)\n",
    "            errors = np.zeros(n_splits)\n",
    "            ei = 0\n",
    "            for train_index, test_index in skf.split(X_train, y_train):\n",
    "                knn = KNNClassifier(X_train[train_index,:], y_train[train_index], np.array(ids_train)[train_index], features, 5 ,\"min_max\")\n",
    "                cm, cm_list, er = knn.evaluate(X_train[test_index,:].copy(), y_train[test_index].copy(), np.array(ids_train)[test_index].copy())\n",
    "                errors[ei] = er\n",
    "                ei = ei + 1\n",
    "\n",
    "            avg_error = np.average(errors)\n",
    "            if avg_error < best_error_rate:\n",
    "                best_error_rate = avg_error\n",
    "                chosen_features = features\n",
    "    return chosen_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d21eef",
   "metadata": {},
   "source": [
    "Run the function:\n",
    "\n",
    "**NB - Can take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a41214",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = k_fold_cross_validation(feature_pool_1_combinations, feature_pool_2)\n",
    "print(\"Selected features: \")\n",
    "print(chosen_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc08c08",
   "metadata": {},
   "source": [
    "Evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,chosen_features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,chosen_features, genres, \"Test\")\n",
    "\n",
    "knn = KNNClassifier(X_train, y_train, ids_train,chosen_features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test, y_test, np.array(ids_test).copy())\n",
    "\n",
    "print(\"Error rate: \", er)\n",
    "\n",
    "# Plot confusion matrices\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84055325",
   "metadata": {},
   "source": [
    "#### ANOVA f value feature selection\n",
    "\n",
    "Initialise variables and calculate scores for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pool_2 = [\"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "\"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "\"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "\"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "\"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "\"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "\"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "\"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "\n",
    "# Get points and classes\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,feature_pool_2, genres, \"Train\")\n",
    "\n",
    "# Define k_best object\n",
    "k_best = SelectKBest(k=\"all\")\n",
    "k_best = k_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76bd3e",
   "metadata": {},
   "source": [
    "Find the best features that uses at least 3 from the given features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for each feature and index\n",
    "scores = np.array(k_best.scores_)\n",
    "scores_and_index = [(i, scores[i]) for i in range(len(scores))]\n",
    "scores_and_index = sorted(scores_and_index, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "# Find best features\n",
    "chosen_features = []\n",
    "joker_taken = False\n",
    "\n",
    "for si in scores_and_index:\n",
    "    idx = si[0]\n",
    "    feature = feature_pool_2[idx]\n",
    "\n",
    "    if feature in feature_pool_1:\n",
    "        chosen_features.append(feature)\n",
    "    elif joker_taken is False: \n",
    "        chosen_features.append(feature)\n",
    "        joker_taken = True\n",
    "\n",
    "    if len(chosen_features) == 4:\n",
    "        break\n",
    "\n",
    "print(\"Chosen featues: \", chosen_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824d296",
   "metadata": {},
   "source": [
    "Evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa03172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,chosen_features, genres, \"Train\")\n",
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,chosen_features, genres, \"Test\")\n",
    "\n",
    "knn = KNNClassifier(X_train, y_train, ids_train,chosen_features, 5 ,\"min_max\")\n",
    "cm, cm_list, er = knn.evaluate(X_test, y_test, np.array(ids_test).copy())\n",
    "\n",
    "print(\"Error rate: \", er)\n",
    "\n",
    "# Plot confusion matrices\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4682403",
   "metadata": {},
   "source": [
    "### Part 4: A general classifier\n",
    "\n",
    "Two methods were implemented: maximum log-likelihood of Gaussian mixture models of the data, using feature selection, and a multi layered perceptron, using all features. \n",
    "\n",
    "#### Gaussian mixture model classifier\n",
    "The training to find the best GM takes some time, so this will not be implemented here. This is implemented in task4_GM_trainer.py in the scripts folder. Here we will implement the evaluation of the final GM, when the number of components and number of best features to use have been found.\n",
    "\n",
    "Again we first initialise features, genres and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features and genres:\n",
    "features    = [ \"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "                \"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "                \"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "                \"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "                \"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "                \"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "                \"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "                \"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "genres          = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]\n",
    "\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de670c98",
   "metadata": {},
   "source": [
    "Normalise and find k best features, where k = 14 has been found from training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20582f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scalers and kbest transform\n",
    "scaler  = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "k_best  = SelectKBest(k=14)\n",
    "X_train = k_best.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c0153",
   "metadata": {},
   "source": [
    "Define component size of models and create models:\n",
    "\n",
    "*Most likely a warning will pop up, ignore this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01be4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_sizes = {'pop': 1, 'metal': 1, 'disco': 8, 'blues':2, 'reggae':1, 'classical':1, 'rock': 4, 'hiphop': 6, 'country': 1, 'jazz':3}\n",
    "\n",
    "# Init dict for best models for each genre\n",
    "best_gmms = dict() # Dict from genre to best gmm for that genre\n",
    "\n",
    "# Create appropriate model\n",
    "for genre in genres:\n",
    "    # Here we fit a GM\n",
    "    X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, [genre], \"Train\")\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_train = k_best.transform(X_train)\n",
    "    gmm = GaussianMixture(n_components=model_sizes[genre], covariance_type=\"full\",init_params='kmeans',random_state=1)\n",
    "    gmm.fit(X_train)\n",
    "    best_gmms[genre] = copy.deepcopy(gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5e211",
   "metadata": {},
   "source": [
    "Evaluate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set\n",
    "X_test, y_test, ids  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "# Scale and extract k best features\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = k_best.transform(X_test)\n",
    "\n",
    "# Compute log likelihood for every function\n",
    "log_likelihood = np.zeros((len(y_test),len(genres)))\n",
    "for genre, model in best_gmms.items():\n",
    "    log_likelihood[:,genres.index(genre)]  = model.score_samples(X_test)\n",
    "\n",
    "# Find index of best log likelihood\n",
    "maxVal_ind_rowise = np.argmax(log_likelihood, axis=1)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = np.zeros((len(genres), len(genres)))\n",
    "for i in range(len(y_test)):\n",
    "    cm[genres.index(genre_id_to_string(y_test[i])), maxVal_ind_rowise[i]] += 1\n",
    "\n",
    "# Display confusion matrices\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=genres)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"Error rate: \", error_rate(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7890d66",
   "metadata": {},
   "source": [
    "#### Multilayer perceptron\n",
    "Again the model finding procedure took some time. The file task4_NN_trainer.py holds the training algorithm. Here we implement the final network, and evaluate it. \n",
    "\n",
    "Again we start with defining features and genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32516cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = [ \"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "                \"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "                \"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "                \"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "                \"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "                \"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "                \"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "                \"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "genres      = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3faead",
   "metadata": {},
   "source": [
    "Get the right data and scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c24914",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9d12d",
   "metadata": {},
   "source": [
    "Create classifier and fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,activation=\"logistic\", max_iter=100000,verbose=False,\n",
    "                                hidden_layer_sizes=(40,), random_state=1)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412a386",
   "metadata": {},
   "source": [
    "Evaluate classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5890f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Error rate: \", error_rate(cm))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=genres)\n",
    "disp.plot(cmap=plt.cm.Blues,xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2162a3",
   "metadata": {},
   "source": [
    "*Extra task: find suitable alpha value*\n",
    "\n",
    "First we initialise some variables and get the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3616b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features and genres:\n",
    "features    = [ \"zero_cross_rate_mean\",\"zero_cross_rate_std\",\"rmse_mean\",\"rmse_var\",\"spectral_centroid_mean\",\"spectral_centroid_var\",\"spectral_bandwidth_mean\",\n",
    "                \"spectral_bandwidth_var\",\"spectral_rolloff_mean\",\"spectral_rolloff_var\",\"spectral_contrast_mean\",\"spectral_contrast_var\",\"spectral_flatness_mean\",\n",
    "                \"spectral_flatness_var\",\"chroma_stft_1_mean\",\"chroma_stft_2_mean\",\"chroma_stft_3_mean\",\"chroma_stft_4_mean\",\"chroma_stft_5_mean\",\"chroma_stft_6_mean\",\n",
    "                \"chroma_stft_7_mean\",\"chroma_stft_8_mean\",\"chroma_stft_9_mean\",\"chroma_stft_10_mean\",\"chroma_stft_11_mean\",\"chroma_stft_12_mean\",\"chroma_stft_1_std\",\n",
    "                \"chroma_stft_2_std\",\"chroma_stft_3_std\",\"chroma_stft_4_std\",\"chroma_stft_5_std\",\"chroma_stft_6_std\",\"chroma_stft_7_std\",\"chroma_stft_8_std\",\"chroma_stft_9_std\",\n",
    "                \"chroma_stft_10_std\",\"chroma_stft_11_std\",\"chroma_stft_12_std\",\"tempo\",\"mfcc_1_mean\",\"mfcc_2_mean\",\"mfcc_3_mean\",\"mfcc_4_mean\",\"mfcc_5_mean\",\"mfcc_6_mean\",\n",
    "                \"mfcc_7_mean\",\"mfcc_8_mean\",\"mfcc_9_mean\",\"mfcc_10_mean\",\"mfcc_11_mean\",\"mfcc_12_mean\",\"mfcc_1_std\",\"mfcc_2_std\",\"mfcc_3_std\",\"mfcc_4_std\",\"mfcc_5_std\",\n",
    "                \"mfcc_6_std\",\"mfcc_7_std\",\"mfcc_8_std\",\"mfcc_9_std\",\"mfcc_10_std\",\"mfcc_11_std\",\"mfcc_12_std\"]\n",
    "genres      = [\"pop\",\"metal\", \"disco\", \"blues\", \"reggae\", \"classical\", \"rock\", \"hiphop\", \"country\", \"jazz\"]\n",
    "\n",
    "# Extract training\n",
    "X_train, y_train, ids_train  = getPointsAndClasses(songs_dict,features, genres, \"Train\")\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91301a",
   "metadata": {},
   "source": [
    "Initialise arrays for error rate and choose alphas to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b01778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise arrays for error rates\n",
    "error_rates_train = []\n",
    "error_rates_test = []\n",
    "\n",
    "# Make list of alphas to test\n",
    "alphas = np.logspace(-5,2,num=8, base = 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe423fc",
   "metadata": {},
   "source": [
    "Test all alphas and append to values to arrays:\n",
    "\n",
    "**NB! Can take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57994327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for each alpha\n",
    "for alpha in alphas:\n",
    "    #Create classifier\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=alpha,activation=\"logistic\", max_iter=100000,verbose=True,\n",
    "                                    hidden_layer_sizes=(40,), random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate for alpha\n",
    "    X_test, y_test, ids_test  = getPointsAndClasses(songs_dict,features, genres, \"Test\")\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    error_rates_test.append(error_rate(cm_test))\n",
    "    error_rates_train.append(error_rate(cm_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66342ef",
   "metadata": {},
   "source": [
    "Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(alphas, np.array(error_rates_train), color=\"red\",label=\"Training set\")\n",
    "ax.plot(alphas, np.array(error_rates_test), color=\"blue\", label=\"Test set\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"\\u03B1\")\n",
    "ax.set_ylabel(\"Error rate\")\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
